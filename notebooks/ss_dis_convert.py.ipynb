{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start coding here\n",
    "# files\n",
    "input_ss_dis = snakemake.input.ss_dis\n",
    "output_ss_dis = snakemake.output.reformatted_ss_dis\n",
    "_tmp = \"tmp/ss_dis_interim.txt\"\n",
    "\n",
    "# sourced: https://alexwlchan.net/2018/12/iterating-in-fixed-size-chunks/\n",
    "import itertools\n",
    "\n",
    "\n",
    "def chunked_iterable(iterable, size):\n",
    "    it = iter(iterable)\n",
    "    while True:\n",
    "        chunk = tuple(itertools.islice(it, size))\n",
    "        if not chunk:\n",
    "            break\n",
    "        yield chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Overall changes\n",
    "# moved secstr_disorder_merge fcn to this file\n",
    "# changed hardcoded paths to variables (defined from snakemake in above cell)\n",
    "# \n",
    "\n",
    "# %load scripts/ss_dis_convert.py\n",
    "from Bio import SeqIO\n",
    "import json\n",
    "\n",
    "\n",
    "def secstr_disorder_merge(secstr, disorder):\n",
    "    #replace all spaces with dashes\n",
    "    #import pdb; pdb.set_trace()\n",
    "    replacement = list(secstr) ## removed replace, this is already addressed in create_dict by making temp file\n",
    "    #if there should be an X, replace the dash with an X\n",
    "    for i in range(len(replacement)):\n",
    "        if disorder[i] == \"X\":\n",
    "            replacement[i] = \"X\"\n",
    "    replacement_str = ''.join(replacement)\n",
    "    return replacement_str\n",
    "\n",
    "def create_dict():\n",
    "    \"\"\" ## moving data to near block where it is populated\n",
    "    data = {}\n",
    "    record_list = []\n",
    "    \"\"\"\n",
    "    record_count = 0\n",
    "    \n",
    "    print(f\"Starting to read {input_ss_dis} and write temporary file {_tmp}...\")\n",
    "    #replace all the spaces in the file and replaces it with dashes\n",
    "    #so that no information goes missing when parsing through the file\n",
    "    replace = open(_tmp, \"w\") ## w+ -> w : w+ allows read and write, we only need to write the tmp file\n",
    "    with open(input_ss_dis,\"r\") as in_file: ## r+ -> r : r+ is allows reading and writing access, we only need reading\n",
    "        for line in in_file:\n",
    "            if \">\" in line:\n",
    "                record_count += 1\n",
    "            fixed_line = line.replace(\" \", \"L\")\n",
    "            replace.write(fixed_line)\n",
    "    replace.close()\n",
    "    print(\"Success\")\n",
    "\n",
    "    print(\"Now to parse through the whole thing....\")\n",
    "    # After all the spaces have been replaced, move on\n",
    "    \"\"\" ## reworked, see below in line comments for rationale \n",
    "\n",
    "    record_dict = SeqIO.to_dict(SeqIO.parse(\"tmp/ss_dis_interim.txt\", \"fasta\")) ## running the Parse twice is very expensive\n",
    "    ## also when possible, avoid preloading entire list from generator, get to know difference between lists and generators-very important for speeding up many iterable processes \n",
    "    for record in SeqIO.parse(\"tmp/ss_dis_interim.txt\", \"fasta\"): ## second Parse run\n",
    "        #creates unique id for dictionary\n",
    "        id = record.id.split(\":\")[0] + \":\" + record.id.split(\":\")[1]\n",
    "        #if unique, add to list\n",
    "        if(not (id in record_list)): ## clarity rewrite: 'if id not in record_list:'\n",
    "            record_list.append(id) ## alternatively, don't check for uniqueness, instead run 'list(set(record_list))', this will convert the list to a set (which removes redudant items) then back to a list which now only contains unique values\n",
    "    print(\"Success\")\n",
    "\n",
    "    print(\"Finally, to add everything into a dictionary...\")\n",
    "    for id in record_list: ## rather than checking through the record descriptions, we should exploit the guaranteed triplet sequence format\n",
    "        #create new id for future use\n",
    "        new_id = id.split(\":\")[0] + id.split(\":\")[1] \n",
    "        #find the sequence, secstr, and disorder sequences from dictionary\n",
    "        sequence = str(record_dict[id + \":sequence\"].seq)\n",
    "        secstr = str(record_dict[id + \":secstr\"].seq)\n",
    "        disorder = str(record_dict[id + \":disorder\"].seq)\n",
    "        merge = str(secstr_disorder_merge(secstr, disorder))\n",
    "        #add all information into useful dictionary\n",
    "        data[new_id] = sequence, merge\n",
    "    \"\"\"\n",
    "    ## Reworked version of above block\n",
    "    data = dict()\n",
    "    triplet_records = chunked_iterable(iterable=SeqIO.parse(_tmp,\"fasta\"), size=3)\n",
    "    for i, (primary, secstruct, disorder) in enumerate(triplet_records):\n",
    "        print(f\"Coverting record {i+1} of {int(record_count/3)} records {' '*8}\",end=\"\\r\")\n",
    "        merge = secstr_disorder_merge(secstr=str(secstruct.seq), disorder=str(disorder.seq))\n",
    "        new_id = primary.id.split(\":\")[0] + primary.id.split(\":\")[1]\n",
    "        data[new_id] = str(primary.seq), merge\n",
    "        \n",
    "    print(\"Success\")\n",
    "    \n",
    "\n",
    "    print(\"Finally to create the JSON File...\")\n",
    "    with open(output_ss_dis, \"w\") as file: ## w+ -> w\n",
    "            json.dump(data,file, indent=4)\n",
    "    print(\"Finished!\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 1 -r 1\n",
    "# run the functions above\n",
    "\n",
    "create_dict()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
